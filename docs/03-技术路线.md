# 营销文案生成 AI Agent - 技术路线

## 1. 技术栈概览

| 层级 | 技术 | 用途 |
|------|------|------|
| 编排 | LangGraph | Agent 图编排、状态管理、条件路由 |
| LLM 集成 | LangChain | 模型抽象，通过 OpenAI 兼容 API 调用 |
| 本地部署 | vLLM 等 | 主 LLM、增强 LLM、多模态 LLM 分别本地部署；Agent 直接调用 |
| 图片生成 | 本地多模态 LLM | 由 vLLM 部署的模型处理；Agent 不关心部署细节 |
| 安全校验 | 规则 + 本地 LLM | 输入/输出合规校验 |
| MCP 集成 | langchain-mcp-adapters | 连接 MCP Server，转换为 LangChain Tools |
| API 服务 | FastAPI + Uvicorn | HTTP 接口、异步处理、文件上传 |
| 图片处理 | Pillow | 图片解码、尺寸校验、格式转换 |
| 配置 | pydantic-settings | 环境变量、配置校验 |
| 日志 | structlog | 结构化日志 |
| 重试 | tenacity | LLM/MCP 调用重试 |

> **约束**：不调用外部 API；所有模型本地部署。
> **模块化**：Skills、阶段上下文、MCP Servers 均配置驱动，便于扩展。

---

## 2. 实施阶段

### 阶段 1：基础搭建（1–2 周）

#### 2.1 目标

- 搭建项目结构
- 实现最小可运行 LangGraph
- 暴露基础 FastAPI 接口

#### 2.2 任务

1. **项目骨架**
   - 创建 `promotional_copy_generation/` 包结构
   - 创建 `config/` 目录及 `stage_contexts.yaml`、`mcp_servers.yaml` 模板
   - 创建 `skills/registry.py`、`skills/loader.py`、`mcp/loader.py` 模块化加载框架
   - 配置 `pyproject.toml` 的 `[tool.uv.scripts]` 或入口点
   - 配置环境变量与 `.env.example`

2. **AgentState 与最小图**
   - 定义 `AgentState`（初期含 requirements、description、final_copy）
   - 实现单节点 `CopyWriter`（调用本地 vLLM 等生成文案）
   - 构建 START → CopyWriter → END 的 `StateGraph`
   - `graph.compile()` 得到可调用应用

3. **FastAPI 入口**
   - `POST /generate`：接收 JSON，调用 agent.invoke，返回文案
   - `GET /health`：健康检查
   - 使用 uvicorn 启动

#### 2.3 验收

- 可通过 curl 调用 `/generate` 并获取文案
- 日志可追踪请求与 LLM 调用

---

### 阶段 2：上下文增强与图片理解（1–2 周）

#### 2.1 目标

- 支持描述模糊时的上下文增强
- 支持输入图片并生成描述

#### 2.2 任务

1. **上下文增强节点**
   - 实现 `ContextEnhancer` 节点
   - 输入：raw_requirements, raw_description, input_images（可选）
   - 输出：enhanced_context
   - 多模态：有输入图时支持文+图生文，通过 `build_human_message` 传入图像
   - 条件路由：根据描述质量决定是否进入该节点

2. **上下文增强融合图像决策**
   - context_enhancer 单次 LLM 调用同时输出 enhanced_context 与 need_image_generation
   - 有输入图：need_image_generation = True
   - 无输入图：LLM 在增强时一并判断是否需要生成图像

3. **图结构更新**
   - 流程：context_enhance → copy_write（need_image 由 context_enhance 输出）
   - 条件边：need_image_generation 为真时执行 image_prompt → image_gen

4. **状态扩展**
   - 在 AgentState 中新增 enhanced_context、need_image_generation

#### 2.3 验收

- 模糊描述经增强后更清晰
- 有输入图通常生成配图，除非用户明确/暗示仅要纯文本；无图时 LLM 正确判断

---

### 阶段 3：Skills 与图片生成（1–2 周）

#### 2.1 目标

- 集成 Skills（文案、平台、图片）
- 生成图片提示词
- 调用本地多模态 LLM 输出图片

#### 2.2 任务

1. **Skills 集成（模块化）**
   - 实现 `skills/registry.py` 与 `loader.py`，支持按阶段加载
   - 在 `skills/builtin/` 下创建 copy_writing、platform_rules、image_prompt
   - 节点通过 `get_skills_for_stage(stage)` 获取上下文，注入系统提示
   - 支持 `SKILLS_DIRS` 环境变量指定外部 Skill 目录

2. **图片提示词节点**
   - 实现 `ImagePromptGenerator` 节点
   - 输入：raw_requirements, enhanced_context, input_images（可选）
   - 输出：image_prompts（list[str]）
   - 多模态：有输入图时支持文本+图→文本
   - 使用 image-prompt Skill 约束格式与风格

3. **图片生成节点**
   - 实现 `ImageGenerator` 节点（文生图 / 文+图生图）
   - 调用图像生成 API；配置 `IMAGE_GEN_ENABLED` 启用
   - 有输入图时作为参考图做 img2img 风格迁移
   - 输出：generated_images（list[bytes]）

4. **条件路由**
   - 根据 need_image_generation（动态决策结果）路由到 ImagePromptGen → ImageGen

#### 2.3 验收

- Skill 内容影响生成质量
- 可生成多张图片
- 支持「输入图片 + 描述」的增强生成

---

### 阶段 5：安全校验与并行负载（1–2 周）

#### 2.1 目标

- 实现输入/输出合规安全校验
- 支持并行请求
- 控制并发与资源
- 提升可观测性与错误处理

#### 2.2 任务

1. **安全校验模块**
   - 实现 `InputSafetyChecker`：校验需求、描述、图片合规
   - 实现 `OutputSafetyChecker`：校验文案、图片提示词、生成图片
   - 校验项：违禁词、广告法、平台规范、虚假宣传、违规图片
   - 实现：规则引擎 + 违禁词表 + 本地 LLM 分类（可选）
   - 不合规：拒绝输入请求；拦截或重试输出

2. **并发控制**
   - 使用 `asyncio.Semaphore` 限制并发 agent 调用
   - 或引入任务队列处理异步长任务

3. **流式输出（可选）**
   - `POST /generate/stream`：使用 `agent.stream()` 流式返回文案
   - 使用 Server-Sent Events（SSE）

4. **可观测性**
   - 集成 LangSmith（可选）做调用追踪
   - 使用 structlog 记录请求 ID、节点、耗时、Token 数
   - 添加 Prometheus 指标（可选）

5. **错误处理**
   - LLM 超时、限流：tenacity 重试
   - MCP 失败：降级策略
   - 图片生成失败：仅返回文案，不含图片；记录错误

6. **部署**
   - Dockerfile
   - docker-compose（含环境变量）
   - 健康检查与就绪探针

#### 2.3 验收

- 多并发请求可稳定处理
- 关键错误有日志与降级
- 可容器化部署

---

## 3. 依赖安装与验证

### 3.1 已安装依赖（pyproject.toml）

```toml
dependencies = [
    "langgraph>=0.2.0",
    "langchain>=0.3.0",
    "langchain-core>=0.3.0",
    "langchain-community>=0.3.0",   # 本地模型访问（vLLM 通过 OpenAI 兼容 API）
    "langchain-mcp-adapters>=0.2.0",
    "fastapi>=0.115.0",
    "uvicorn[standard]>=0.32.0",
    "python-multipart>=0.0.12",
    "pydantic>=2.10.0",
    "pydantic-settings>=2.6.0",
    "pillow>=11.0.0",
    "httpx>=0.28.0",
    "aiofiles>=24.1.0",
    "structlog>=24.4.0",
    "tenacity>=9.0.0",
]
```

### 3.2 验证

```bash
uv sync
uv run python -c "
from langgraph.graph import StateGraph
from langchain_openai import ChatOpenAI  # base_url 指向 vLLM
print('LangGraph + LangChain OK')
from langchain_mcp_adapters import MultiServerMCPClient
print('MCP Adapters OK')
from fastapi import FastAPI
print('FastAPI OK')
"
```

### 3.3 本地模型部署要求

- **vLLM 等**：主 LLM、增强 LLM、多模态 LLM 分别本地部署；Agent 直接调用 API；部署细节抽象
- **安全校验**：规则（违禁词）+ LLM 合规判断；可配置 SAFETY_USE_LLM 开关

---

## 4. 风险与应对

| 风险 | 应对措施 |
|------|----------|
| 小红书 MCP 无现成实现 | 先实现 mock MCP；后续替换为真实服务 |
| 本地模型显存不足 | 使用更小模型；文案生成与图片理解分开部署 |
| 多模态 LLM 延迟高 | 使用流式输出；缓存图片理解结果 |
| MCP 连接不稳定 | 重试 + 超时 + 降级；不阻塞主流程 |
| 安全校验漏检 | 规则 + 本地 LLM 双重校验；定期更新违禁词 |

---

## 5. 里程碑时间线

| 里程碑 | 预估时间 | 交付物 |
|--------|----------|--------|
| M1：基础搭建 | 第 1–2 周 | 最小可调用 Agent + FastAPI（调用本地 vLLM 等） |
| M2：增强与图片理解 | 第 3–4 周 | 上下文增强 + 图片理解（本地多模态） |
| M3：MCP 集成 | 第 5–6 周 | 参考检索 + 降级策略 |
| M4：Skills 与图片 | 第 7–8 周 | Skills + 图片提示词 + 多模态 LLM 图片生成 |
| M5：安全校验与生产化 | 第 9–10 周 | 输入/输出安全校验 + 并发、监控、部署 |

---

## 6. 参考资料

- [LangGraph 文档](https://langchain-ai.github.io/langgraph/)
- [LangChain MCP Adapters](https://github.com/langchain-ai/langchain-mcp-adapters)
- [MCP 协议规范](https://modelcontextprotocol.io/)
- [FastAPI 文档](https://fastapi.tiangolo.com/)
